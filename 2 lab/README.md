# Описание структуры работы над лабораторной

## Дерево решений

### 1.      Алгоритм работы дерева решений

Работа алгоритма заключается в следующем (взято честно из статьи [2]):

      s0 = вычисляем энтропию исходного множества
      
      Если s0 == 0 значит:
            Все объекты исходного набора, принадлежат к одному классу
            Сохраняем этот класс в качестве листа дерева
            
      Если s0 != 0 значит:
            Перебираем все элементы исходного множества:
                  Для каждого элемента перебираем все его атрибуты:
                        На основе каждого атрибута генерируем предикат, который разбивает исходное множество на два подмножества
                        Рассчитываем среднее значение энтропии
                        Вычисляем ∆S
            Нас интересует предикат, с наибольшим значением ∆S
            Найденный предикат является частью дерева принятия решений, сохраняем его
            
            Разбиваем исходное множество на подмножества, согласно предикату
            Повторяем данную процедуру рекурсивно для каждого подмножества

Вычисление энтропии производится по следующей формуле [2]:

![Формула вычисления энтропии](https://github.com/Knzaytsev/IntroductionToML/raw/master/2%20lab/img/entropy.png)

∆S - это Information Gain, вычисляемый по формуле [1]:

![Формула вычисления Information Gain](https://github.com/Knzaytsev/IntroductionToML/raw/master/2%20lab/img/information%20gain.png),

где S - это энтропии, q - количество групп, N - общее число элементов


### 2.      Полезные ссылки
1.    https://habr.com/ru/company/ods/blog/322534/
2.    https://habr.com/ru/post/171759/
3.    https://edu.kpfu.ru/pluginfile.php/91556/mod_resource/content/3/Decision%20trees_1.pdf
4.    https://www.hse.ru/mirror/pubs/share/215285956
5.    https://ranalytics.github.io/data-mining/052-Binary-Decision-Trees.html
6.    https://learnmachinelearning.wikia.org/ru/wiki/Решающее_дерево_(Decision_tree)

## Случайный лес

### 1.      Алгоритм работы случайного леса

      Для каждого n = 1, ..., N:
            Сгенерировать выборку Xn с помощью бутстрэпа;
            Построить решающее дерево bn по выборке :
                  — по заданному критерию мы выбираем лучший признак, делаем разбиение в дереве по нему и так до исчерпания выборки
                  — дерево строится, пока в каждом листе не более nmin объектов или пока не достигнем определенной высоты дерева
                  — при каждом разбиении сначала выбирается m случайных признаков из n исходных,
                  и оптимальное разделение выборки ищется только среди них.

### 2.      Полезные ссылки
1.    https://habr.com/ru/company/ods/blog/324402/#algoritm

## Градиентный бустинг

### 1.      Алгоритм работы градиентного бустинга

Алгоритм работы градиентного бустинга выглядит следующим образом (взято из [1]):

![Алгоритм градиентного бустинга](https://github.com/Knzaytsev/IntroductionToML/raw/master/2%20lab/img/gbm.png)

Loss-функция будет вычисляться таким образом:

![Формула вычисления функции потерь](https://github.com/Knzaytsev/IntroductionToML/raw/master/2%20lab/img/loss.png)

### 2.      Полезные ссылки

1.    https://habr.com/ru/company/ods/blog/327250/
2.    https://habr.com/ru/company/mailru/blog/438562/
